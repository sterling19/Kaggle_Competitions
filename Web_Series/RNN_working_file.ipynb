{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "''' Loading packages '''\n",
    "import gc\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "''' Loading data '''\n",
    "FILE_PATH = \".//train_test//\"\n",
    "\n",
    "TRAIN_FILE = \"en_train.csv\"\n",
    "TEST_FILE = \"en_test.csv\"\n",
    "\n",
    "train_set = pd.read_csv(FILE_PATH + TRAIN_FILE)\n",
    "test_set = pd.read_csv(FILE_PATH + TEST_FILE)\n",
    "\n",
    "# Dropping language columns\n",
    "train_set.drop('lang', axis=1, inplace=True)\n",
    "test_set.drop('lang', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' Selecting number of days for training '''\n",
    "NUM_DAYS = 180\n",
    "\n",
    "date_range = train_set.columns[-NUM_DAYS:].tolist()\n",
    "date_range.insert(0, 'Page')\n",
    "train_set = train_set.loc[:, date_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "''' Replacing null values with median or logmean '''\n",
    "def logmean(x):\n",
    "    return np.expm1(np.mean(np.log1p(x)))\n",
    "\n",
    "\n",
    "def fill_median(): \n",
    "    impute_values = np.empty(shape=(train_set.shape[0],))\n",
    "    data_values = np.array(train_set.iloc[:, 1:])\n",
    "    MEDIAN = np.nanmedian(data_values.flatten())\n",
    "    \n",
    "    for row in range(train_set.shape[0]):\n",
    "        if np.sum(np.isnan(data_values[row])) == len(data_values[row]):\n",
    "            impute_values[row] = MEDIAN\n",
    "        else:\n",
    "            impute_values[row] = np.nanmedian(data_values[row])\n",
    "      \n",
    "    return(impute_values)\n",
    "\n",
    "\n",
    "def fill_logmean():\n",
    "    impute_values = np.empty(shape=(train_set.shape[0],))\n",
    "    data_values = np.array(train_set.iloc[:, 1:])\n",
    "    LOGMEAN = logmean(data_values.flatten())\n",
    "    \n",
    "    for row in range(train_set.shape[0]):\n",
    "        if np.sum(np.isnan(data_values[row])) == len(data_values[row]):\n",
    "            impute_values[row] = LOGMEAN\n",
    "        else:\n",
    "            data_values[row] = data_values[row][~np.isnan(data_values[row])]\n",
    "            impute_values[row] = logmean(data_values[row])\n",
    "\n",
    "    return(impute_values)\n",
    "\n",
    "\n",
    "# train_set['impute_values'] = fill_median()\n",
    "train_set['impute_values'] = fill_logmean()\n",
    "\n",
    "train_set.iloc[:, 1:] = train_set.iloc[:, 1:].apply(lambda x: x.fillna(value=train_set['impute_values']))\n",
    "train_set.drop('impute_values', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Applying Min/Max Scaler '''\n",
    "sc = MinMaxScaler()\n",
    "sc.fit(train_set.iloc[:, 1:].values)\n",
    "train_set_sc = pd.DataFrame(sc.transform(train_set.iloc[:, 1:].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' Clean Memory '''\n",
    "del train_set\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# RNN Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rnn_model():\n",
    "    \n",
    "    # Model architecture\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units=128, input_dim=179, activation='tanh', dropout=0.2, recurrent_dropout=0.0))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # setup predictions array\n",
    "    predictions = np.empty(shape=(train_set.shape[0], 60))\n",
    "    \n",
    "    # train/test split\n",
    "    X_train = train_set_sc.iloc[:, 0:179]\n",
    "    y_train = train_set_sc.iloc[:, 179] # 12/31/16\n",
    "    X_test = train_set_sc.iloc[:, 1:180]\n",
    "    \n",
    "    # predict one day at a time\n",
    "    for i in range(60):\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            print('Predicting Day {0}'.format(i))\n",
    "\n",
    "        # reshape data for neural net\n",
    "        X_tr = X_train.values.reshape(-1, 1, 179)\n",
    "        X_te = X_test.values.reshape(-1, 1, 179)\n",
    "        y_tr = y_train.values.reshape(-1, 1)\n",
    "        \n",
    "        # train and predict\n",
    "        model.fit(X_tr, y_tr, epochs=50, batch_size=1000, verbose=0)\n",
    "        preds = model.predict(X_te)\n",
    "        \n",
    "        # append predictions and extend train/test\n",
    "        if i == 0: \n",
    "            predictions = pd.DataFrame(preds, columns=['pred_0'])\n",
    "        else: \n",
    "            predictions = pd.concat((predictions, pd.DataFrame(preds, columns=['pred_' + str(i)])), axis=1)\n",
    "        \n",
    "        X_train = pd.concat((X_train.iloc[:, i+1:], predictions), axis=1)\n",
    "        X_test = pd.concat((X_test.iloc[:, i+1:], predictions), axis=1)\n",
    "        y_train = pd.DataFrame(predictions.iloc[:, i], columns=['pred_' + str(i)])\n",
    "    \n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = rnn_model()\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pad and inverse transform predictions\n",
    "pads = pd.DataFrame(np.empty((train_set.shape[0], 120)))\n",
    "padded_preds = pd.concat((pads, preds), axis=1)\n",
    "final_preds = sc.inverse_transform(padded_preds)[:, 120: ]\n",
    "\n",
    "del pads, padded_preds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sure predictions are in a similar range to training data\n",
    "print(train_set.iloc[0, :].values[-10:])\n",
    "print(final_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def page_model():\n",
    "    '''\n",
    "    RNN that models pages sequentially for all days with time-shifted data; does not perform well\n",
    "    '''\n",
    "    \n",
    "    # Neural Network Architecture\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, input_shape=(1, 60)))\n",
    "    model.add(Dense(1, activation='tanh'))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=5, verbose=0)]\n",
    "    \n",
    "    # Collecting pages for looping\n",
    "    unique_pages = np.unique(train_flattened[['Page']].values)\n",
    "\n",
    "    # Setting up array for appending predictions\n",
    "    predictions = np.empty(shape=(train_set.shape[0], 60))\n",
    "    i = 0\n",
    "    \n",
    "    # Looping through unique pages\n",
    "    for page in unique_pages:\n",
    "        \n",
    "        train_page = train_flattened.ix[train_flattened['Page']==page, 2:]\n",
    "        test_page = test_set.loc[test_set['Page']==page, :]\n",
    "        \n",
    "        # Create 60-day shift matrix\n",
    "        for s in range(1,61):\n",
    "            train_page['Visits_{}'.format(s)]=train_page['Visits'].shift(s)\n",
    "        shift_values = train_page.dropna()\n",
    "    \n",
    "        # Split train/test data\n",
    "        X_train = shift_values.drop('Visits', axis=1).values[:-60]\n",
    "        X_test = shift_values.drop('Visits', axis=1).values[-60:]\n",
    "        y_train = shift_values[['Visits']].values[:60]\n",
    "\n",
    "        # Reshape and normalize data for neural network\n",
    "        X_train = X_train.reshape(-1, 1, 60)\n",
    "        X_test = X_test.reshape(-1, 1, 60)\n",
    "\n",
    "        sc = MinMaxScaler(feature_range=(0, 1))\n",
    "        sc.fit(list(X_train.flatten()) + list(y_train.flatten()) + list(X_test.flatten()))\n",
    "        X_train = sc.transform(X_train.flatten()).reshape(-1, 1, 60)\n",
    "        X_test = sc.transform(X_test.flatten()).reshape(-1, 1, 60)\n",
    "        y_train = sc.fit_transform(y_train)\n",
    "        \n",
    "        # Batch training\n",
    "        model.fit(X_train, y_train, epochs=20, batch_size=20000, verbose=0, validation_data=None, callbacks=callbacks)\n",
    "    \n",
    "        # Predict page and append\n",
    "        preds = model.predict(X_test)\n",
    "        predictions[i] = preds.flatten()\n",
    "        \n",
    "        # Tracking progress\n",
    "        if i % 1000 == 0:\n",
    "            print('{0} rows predicted'.format(i))\n",
    "        i += 1\n",
    "    \n",
    "    # Reverse tranform predictions and return output\n",
    "    predictions = sc.inverse_transform(predictions)    \n",
    "    return(predictions)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
